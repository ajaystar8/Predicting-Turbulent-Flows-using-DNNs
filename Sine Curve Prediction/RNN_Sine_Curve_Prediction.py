# -*- coding: utf-8 -*-
"""PART1: RNN_SINE_CURVE_THE_IPR_INTERNSHIP_PROGRAM_FINAL_26_OCTOBER

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cMiXzHDh4CqMrit8-4rHUJ3y_C6V9FxU?usp=sharing
"""

'''
Name: Ajay Rajendra Kumar
7th Semester, B.Tech. Computer Science and Engineering
University: Manipal Institute of Technology, Manipal
Date: 23 October, 2023
'''

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import torch.optim as optim

df = pd.read_csv("/content/sin_dummy_time_series.txt", sep = '\t', header = None)

# A 80-20 split of the available dataset
train_test_split = round(0.8 * len(df))
train_dataset = df.iloc[:train_test_split]
test_dataset = df.iloc[train_test_split:]

X_train, y_train = train_dataset[0], train_dataset[1]
X_test, y_test = test_dataset[0], test_dataset[1]

len(train_dataset)

fig, ax = plt.subplots(1,1,figsize=(15,4))
ax.plot(X_train, y_train, lw = 3, label = 'train data')
ax.plot(X_test, y_test, lw = 3, label = 'test data')
ax.legend(loc = 'lower left')
plt.show()

# Univariate LSTM, hence no need of X_train => Future values predicted using past values of sin(x) viz. y
train_series = torch.from_numpy(y_train.values)
test_series = torch.from_numpy(y_test.values)

# Univariate LSTM
# 1 value predicted using 20 old values of y
look_back = 20
num_features = 1

train_dataset = []
train_labels = []

# Creating sequences
for i in range(len(train_series) - look_back):
  train_dataset.append(train_series[i: i+look_back])
  train_labels.append(train_series[i+look_back])

# Setting dimensions so that it is ready to be fed into the LSTM
train_dataset = torch.stack(train_dataset).unsqueeze(0)
train_labels = torch.stack(train_labels).unsqueeze(0).unsqueeze(2)

train_dataset.shape

# The LSTM model
'''
Hidden Size: AKA long-term-memory. Long term memory is an array of values.
The hidden_size parameter represents the number of values in this array. More
the number of values, better the model will be able to learn the complex
patterns of the input data.
'''
class LSTM_NN(nn.Module):
  def __init__(self, num_neurons, input_size):
    super(LSTM_NN, self).__init__()
    self.lstm = nn.LSTM(input_size = input_size, hidden_size = num_neurons,
                        batch_first = True)
    self.fc = nn.Linear(num_neurons, 1)

  def forward(self, x):
    out, _ = self.lstm(x)
    out = self.fc(out)
    return out

# Hyperparameter. Tuned to give best results
num_neurons = 4

model = LSTM_NN(num_neurons = num_neurons, input_size = look_back).double()
loss_function = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr = 0.01)

model.train()

for epoch in range(100):
  model.zero_grad()
  predictions = model(train_dataset)
  loss = loss_function(predictions, train_labels)
  loss.backward()
  optimizer.step()

  if epoch % 5 == 0:
    print(f"epoch: {epoch: 4} loss: {loss.item():10.8f}")

# LOSS: 2.1e-6

test_dataset = [test_series[i:i+look_back] for i in range(len(test_series) - look_back)]
test_dataset = torch.stack(test_dataset).unsqueeze(0)

test_dataset.shape

with torch.no_grad():
    test_predictions = model(test_dataset).squeeze()

x = X_test[look_back:]

fig, ax = plt.subplots(1, 1, figsize=(15, 5))
ax.plot(X_train,y_train, lw=2, label='train data')
ax.plot(X_test,y_test, lw=3, c='y', label='test data')
ax.plot(x,test_predictions, lw=3, c='r',linestyle = ':', label='predictions')
ax.legend(loc="lower left")
plt.show();

